# python lab2_1.txt

from sklearn.cluster import KMeans, DBSCAN, Birch 
from sklearn import metrics
from torchvision import datasets, transforms
import torch
import torch.utils.data as data_utils
import numpy as np

from torchvision.datasets import MNIST

#загрузить набор данных MNIST
transform = transforms.Compose([
transforms.ToTensor(),
transforms.Normalize((0.1307,), (0.3081,))  #уменьшает влияние масштаба
])

train_set = MNIST(root='./data', train=True, download=True, transform=transform) #датасет, на котором "тренируем" нейронку
test_set = MNIST(root='./data', train=False, download=True, transform=transform) #на этом датасете проверям, насколько хорошо обучили нейронку (через расчет ARI)

train_data = train_set.data.reshape((-1, 28 * 28)) #делаем картинки одного размера
test_data = test_set.data.reshape((-1, 28 * 28))

train_data = train_data.float() / 255.0   #алгоритм так быстрее работает
test_data = test_data.float() / 255.0








"""sample_indices = np.random.choice(len(test_data), size=1000, replace=False)
sample_data = test_data[sample_indices]
sample_labels = torch.tensor(kmeans.predict(sample_data))"""






 #работает оптимально первое место
#использовать алгоритм KMeans для кластеризации данных из train_data
kmeans = KMeans(n_clusters=10, random_state=12).fit(train_data) #12

#Получение прогнозируемых меток
kmeans_predicate_labels = kmeans.predict(test_data) 

#Рассчитываем скорректированный индекс RAND между прогнозируемыми метками и истинными метками
true_labels = test_set.targets
kmeans_ari = metrics.adjusted_rand_score(true_labels, kmeans_predicate_labels)

#Распечатайте скорректированный индекс RAND и центры кластеров
print("kmeans Adjusted Rand Index:", kmeans_ari)




"""  #не работает этот блок
#использовать алгоритм sklearn.cluster.AffinityPropagation для кластеризации данных
#af = AffinityPropagation(preference=-50) #, affinity='euclidean')
#af.fit(all_images)  # fit - Обучаем модель на данных
af = AffinityPropagation(preference=-50, damping=0.5).fit(train_data)

# прогнозируемых меток и кластерных центров
af_predicated_labels = af.predict(test_data) 
af_cluster_centers = af.cluster_centers_

#Рассчитываем скорректированный индекс RAND между прогнозируемыми метками и истинными метками
true_labels = test_set.targets
af_ari = metrics.adjusted_rand_score(true_labels, af_predicated_labels)

#Распечатайте скорректированный индекс RAND и центры кластеров
print("AF  Adjusted Rand Index:", af_ari)
print("Cluster Centers:", af_cluster_centers)
"""


#работает оптимально (я настраивал вручную), второе место
"""dbscan = DBSCAN(eps=5.2, min_samples=6) #на 5.2 оптимален, min_samples- это как раз n из лекции. 6 - оптимален (ari выше) #через чат подобрать оптимальные параметры утром
dbscan.fit(train_data)
dbscan_predicated_labels = dbscan.fit_predict(test_data) 
#dbscan_cluster_centers = dbscan.cluster_centers_
true_labels = test_set.targets
dbscan_ari = metrics.adjusted_rand_score(true_labels, dbscan_predicated_labels)
print("DBSCAN  Adjusted Rand Index:",dbscan_ari)"""


#не работает
"""af = AffinityPropagation().fit(train_data)#preference=-50, damping=0.5).fit(train_data)
af_predicated_labels = af.predict(test_data) 
true_labels = test_set.targets
af_ari = metrics.adjusted_rand_score(true_labels, af_predicated_labels)
print("AF  Adjusted Rand Index:",af_ari)"""

"""#k-means поковырять
#работает оптимально, 0,324    на последнем месте??
birch = Birch(branching_factor = 40, n_clusters = None, threshold = 7.0).fit(train_data)  #branching_factor=40 оптимален, на 7.0 оптимально threshold=0.3, n_clusters=10).fit(train_data)#preference=-50, damping=0.5).fit(train_data)   #через чат подобрать оптимальные параметры утром
birch_predicated_labels = birch.predict(test_data) 
true_labels = test_set.targets
birch_ari = metrics.adjusted_rand_score(true_labels, birch_predicated_labels)
print("Birch  Adjusted Rand Index:",birch_ari)
"""


#после чата сделать вывод и попробовать на экран таки вывести

"""хуже всего себя показал алгоритм DBSCAN (минимальное значение ARI).  Это может быть связано с тем, что DBSCAN эффективно выделяет кластеры с константной плотностью. Изображение цифр не обладает константной плотностью, что приводит к ошибкам кластеризации. Кроме того, еще одна возможная причина плохой работы DBSCAN -  изображения MNIST имеют много шума.  На втором месте оказался алгоритм Birch, а на первом месте - Kmeans. Вероятная причина победы Kmeans связана с тем, что он хорошо работает с данными, которые имеют ярко выраженные кластеры, как в случае с MNIST (кластеры в этом датасете неплохо выражены). Датасет MNIST содержит изображения цифр, и каждая цифра находится в своем кластере. Это позволяет kmeans разделить изображения на кластеры с высокой точностью (относительно других рассмотренных алгоритмов кластеризации).
                                                                                                                                он рассчитывает расстояния между точками в исходном пространстве. В данном случае, каждый пиксель изображения является точкой, а изображения имеют очень высокую размерность (28*28 = 784). Это приводит к проблеме, называемой "проклятие размерности", когда в высокоразмерном пространстве расстояния между точками становятся совершенно непропорциональными и неинформативными. Кроме того, еще одна возможная причина плохой работы dbscan -  изображения MNIST имеют много шума.
"""




