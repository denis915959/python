# python lab4_d.txt

import torch
import torch.nn as nn
import torchvision
import matplotlib.pyplot as plt


train_set = torchvision.datasets.MNIST(root='./data', train=True, transform=torchvision.transforms.ToTensor(), download=True)
test_set = torchvision.datasets.MNIST(root='./data', train=False, transform=torchvision.transforms.ToTensor(), download=True)

# Create data loaders
train_load = torch.utils.data.DataLoader(dataset=train_set, batch_size=64, shuffle=True) #обучающие данные будут разбиты на пакеты по 128 образцов в каждой
test_load = torch.utils.data.DataLoader(dataset=test_set, batch_size=64, shuffle=False)
#Создаём класс Автоэнкодера. Задаём по 3 слоя на энкодер и декодер. Выход энкодера - сигмоида (значение [0,1]).

class Encoder(nn.Module):  #изменено
    def __init__(self):
        super(Encoder, self).__init__()
        self.encoder = nn.Sequential(nn.Linear(28*28, 128), nn.ReLU(), nn.Linear(128, 16), nn.ReLU())
        self.decoder = nn.Sequential(nn.Linear(16, 64), nn.ReLU(), nn.Linear(64, 28*28), nn.Sigmoid())
        
    def forward(self, x):
        x = x.view(x.size(0), -1)
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        decoded = decoded.view(-1, 1, 28, 28)
        return decoded





#Используем оптимайзер Адам и среднеквадратичную ошибку MSE

autoencoder = Encoder()
"""critery = nn.MSELoss()  #torch.nn.CrossEntropyLoss()  #у Дениса другой критерий
optimizer = torch.optim.SGD(autoencoder.parameters(), lr=0.001, momentum=0.9)"""

critery = nn.BCELoss()   #nn.MSELoss()
optimizer = torch.optim.Adam(autoencoder.parameters(), lr=0.001, eps=0.000000001) #0.001
#Тренируем автоэнкодер



epochs_num = 15 #код дениса
for i in range(0, epochs_num):
    for data in train_load:
        img, _ = data
        img = img.to(torch.device('cpu'))
        output = autoencoder(img)
        loss = critery(output, img)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    print('Epoch [{}/{}], Loss: {:.4f}'.format(i+1, epochs_num, loss.item()))


"""for epoch in range(3): #было 10 #не работает
    running_loss = 0.0
    for i, data in enumerate(train_load, 0):
        inputs, labels = data
        optimizer.zero_grad()
        outputs = autoencoder(inputs)
        loss = critery(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
        if (i % 10000 == 6999): #1000 == 999:  # выводим каждую тысячу образцов
            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 1000))
            running_loss = 0.0"""

"""optimizer.zero_grad()
        outputs = autoencoder(data[0])
        loss = critery(outputs, data[1])
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
        if i % 10000 == 6999:
            print(f"[{epoch+1}, {i+1:5d}] loss: {running_loss/1000:.3f}")
            running_loss = 0.0"""

print('Finished Training')



"""num_epochs = 4 #было 10
for epoch in range(num_epochs):
    for data in train_load:
        img, _ = data
        img = img.to(torch.device('cpu'))
        output = autoencoder(img)
        loss = critery(output, img)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))
"""

#Тестируем модель

with torch.no_grad():
    for data in test_load:
        img, _ = data
        img = img.to(torch.device('cpu'))
        output = autoencoder(img)
        loss = critery(output, img)
        fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(25,4))
        for images, row in zip([img, output], axes):
            for img, ax in zip(images, row):
                ax.imshow(img[0], cmap='gray')
                ax.get_xaxis().set_visible(False)
                ax.get_yaxis().set_visible(False)
        fig.suptitle('Original Images and Reconstructed Images')
        plt.show()
        print('Loss on test dataset: {:.8f}'.format(loss.item()))
        break







"""def __init__(self):
        super(Autoencoder, self).__init__()
        self.encoder = nn.Sequential(
            nn.Linear(28*28, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 16),
            nn.ReLU())
        self.decoder = nn.Sequential(
            nn.Linear(16, 64),
            nn.ReLU(),
            nn.Linear(64, 128),
            nn.ReLU(),
            nn.Linear(128, 28*28),
            nn.Sigmoid())"""





"""def __init__(self):
        super(Autoencoder, self).__init__()
        self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)
        self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2)
        self.conv2 = torch.nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)
        self.fc1 = torch.nn.Linear(in_features=12*4*4, out_features=120)
        self.fc2 = torch.nn.Linear(in_features=120, out_features=60)
        self.fc3 = torch.nn.Linear(in_features=60, out_features=10)"""









