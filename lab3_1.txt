# python lab3.txt

import numpy as np
from scipy.stats import norm #для построения нормального распределения
import matplotlib.pyplot as plt
import torch

x1=-6 #границы значений х
x2=6
step=0.1
x_float = np.arange(x1, x2, step)
k=0.5
b=5

#блок восстановления параметров линии
x = torch.linspace(x1, x2, 120).view(120, 1)
b = torch.linspace(b, b, 120).view(-1, 1)
tensor_size = [120, 1]
e = (torch.rand(tensor_size) - 0.5) #генерирует значния от 0 до 1. вычитааем 0.5 и получаем то, что нужно нам
y = k*x + b + e

x_tensor = x.clone().detach().requires_grad_(True)
x_tensor = x_tensor.view(-1, 1)
y_tensor = y.clone().detach().requires_grad_(True).view(120, 1)


k1 = torch.randn(1, 1, requires_grad=True) #это тензор размера 1х1
b1 = torch.randn(1,  requires_grad=True)

#задаем функцию потерь
loss_fn = torch.nn.MSELoss()

optimizer = torch.optim.SGD([k1, b1], lr=0.005) #(0.001), на (0.01 все работает), lr - размер шага оптимизатора (влияет на скорость обучения)

for i in range(1000): #(1000) число итераций надо настраивать, большое может приводить к переобучению, маленькое число - к малой точности
	# сбрасываем градиенты
	optimizer.zero_grad()
	y_pred = x_tensor.mm(k1) + b1  #перемножение двух тензоров (матриц)
	# вычисляем функцию потерь
	loss = loss_fn(y_pred, y_tensor)
	# вычисляем градиенты
	loss.backward()
	# обновляем параметры модели
	optimizer.step()
	# выводим процесс обучения раз в 50 итерций
	if ((i+1) % 50 == 0):  #100
		print('Epoch [{}/{}], loss: {:.8f}'.format(i+1, 1000, loss.item()))  #loss: {:.4f}'.format(i+1, 1000, loss.item()))

print('k1:', k1.item())
print('b1:', b1.item())

y_mashine = k1*x + b1
#конвертируем тензор в массив чисел для построения графика
y_numpy = y_mashine.detach().numpy()  #numpy()
y_float = y_numpy.tolist()

# Создаем график
plt.scatter(x_float, y, marker=".")

plt.plot(x_float, y_float, color='red')
plt.title('Результаты обучения', fontsize=16)
plt.xlabel('Значения x', fontsize=14)
plt.ylabel('Значения y', fontsize=14)
plt.show()