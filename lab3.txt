# python lab3.txt

import numpy as np
from scipy.stats import norm #для построения нормального распределения
import matplotlib.pyplot as plt
import torch


#блок создания датасета с шумом 
#формируем x_int
x_int = np.arange(-3, 3, 0.05)
k=0.5
b=5
"""y0=[]
size=120

for i in range(0, size):
	tmp=b + k*x_int[i]
	y0.append(tmp)
"""

# Вычисляем соответствующие значения плотности вероятности нормального распределения
"""e = norm.pdf(x, 0, 1)
y=[]
minus=False
m=0.5 #без этого коэффициэнта слишком сильное "расширение" графика в середине
for i in range(0, size):
	#print(e[i])
	if(minus==False):
		tmp=y0[i]+m*e[i]
		minus=True
	else:
		tmp=y0[i]-m*e[i]
		minus=False
	y.append(tmp)
"""

#блок восстановления параметров линии

#заново создаем x и y точно такие же для pytorch
x = torch.linspace(-3, 3, 120).view(-1, 1) #-1 означает, что одну размерность надо автоматически вычислить, 1 это вторая размерность
b = torch.linspace(b, b, 120).view(-1, 1)

tensor_size = [120, 1]
e = (torch.rand(tensor_size)/2) - 0.5 #генерирует значния от 0 до 1. вычитааем 0.5 и получаем то, что нужно нам
mean = 5.0
std = 2.0

# Generate a tensor with random values from a normal distribution
e_N = torch.normal(mean=mean, std=std, size=(120,))/10 #torch.randn(120)

# Multiply the tensor by the standard deviation and add the mean
#e_N = e_N * std + mean

#print(e_N)

b=elementwise_product = torch.mul(b, b)

y = k*x + b + e #+ torch.randn(120, 1)*0.1

x_tensor = x.clone().detach().requires_grad_(True)  #tensor(x) # #преобразуем одномерный массив в тензор, понятный torch
x_tensor = x_tensor.view(-1, 1)
y_tensor = y.clone().detach().requires_grad_(True).view(120, 1) #torch.tensor(y).view(120, 1)

#здесь х и у верно строятся

#инициализируем параметры модели
k1 = torch.randn(1, 1, requires_grad=True) #это тензор размера 1х1
b1 = torch.randn(1,  requires_grad=True)
#print("b1 = ", b1)
#задаем функцию потерь
loss_fn = torch.nn.MSELoss()

optimizer = torch.optim.SGD([k1, b1], lr=0.005) #(0.001), на (0.01 все работает), lr - размер шага оптимизатора (влияет на скорость обучения)

for i in range(1000): #(1000) число итераций надо настраивать, большое может приводить к переобучению, маленькое число - к малой точности
	# сбрасываем градиенты
	optimizer.zero_grad()
	y_pred = x_tensor.mm(k1) + b1  #перемножение двух тензоров (матриц)
	"""if(i==0):
		print("start")
		print(y_pred)"""
	# вычисляем функцию потерь
	loss = loss_fn(y_pred, y_tensor)

	# вычисляем градиенты
	loss.backward()

	# обновляем параметры модели
	optimizer.step()

	# выводим процесс обучения
	if ((i+1) % 100 == 0):  #100
		print('Epoch [{}/{}], Loss: {:.4f}'.format(i+1, 1000, loss.item()))

print('k1:', k1.item())
print('b1:', b1.item())

y_mashine = k1*x + b1
# Convert the tensor to a NumPy array
y_numpy = y_mashine.detach().numpy()  #numpy()

# Convert the NumPy array to a list of float numbers
y_float = y_numpy.tolist()

# Создаем график
plt.scatter(x_int, y, marker=".")

plt.plot(x_int, y_float, color='red') #plot - строит ЛИНИИ!!!!!
# Добавляем заголовок и подписи осей
plt.title('Нормальное распределение', fontsize=16)
plt.xlabel('Значения X', fontsize=14)
plt.ylabel('Плотность вероятности', fontsize=14)

# Отображаем график
plt.show()